---
title: "Bidirectional Encoder Representations from Transformers"
date: 2020-03-06
tags: [BERT Model, deep learning, NLP]
header:
excerpt: "BERT Model, deep learning, NLP"
mathjax: "true"
---
<img src="{{ site.url }}{{ site.baseurl }}/images/bert.jpg"  width="1000px" height='800px'/>

BRET is a revolutionary technique that achieved state-of-the-art results on a range of NLP tasks. 
Opposed to almost benchmarks trained with a given task, BRET is a self-supervised method, trained on unannotated text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BRET model can be fine tuned with just on additional output layer to create state-of-the-art models for a wide range of NLP applications such as Q-A models, language inference ... 
There are normally two strategies for applying pre-trained language representations to down stream tasks: 
    . Feature-based : to include the pre-trained representations as additional features. 
    . Fine-tuning : retrain a minimal task-specific parameters on the downstream tasks. 
     
Released by Google in 2018. [BERT](https://github.com/google-research/bert) stands for Bidirectional Encoder Representation from Transformers. The model randomly chooses masked tokens from the input, and tries to predict the original vocabulary Id of these masked tokens based on its context. 

    

RoBERTa - an optimized method for BRET, builds on BRET's language masking strategy, 
wherein the system learns to predict intentionally hidden sections of text within otherwise unannotated language examples. 
It modifies key hyper-parameters in BRET: 
    . Remove BRET's next sentence pre-training objective.   
    . Train with larger mini-batches and learning rates. 