---
title: "Bidirectional Encoder Representations from Transformers"
date: 2020-03-06
tags: [BERT Model, deep learning, NLP]
header:
excerpt: "BERT Model state-of-the-art on a range of NLP task"
mathjax: "true"
---
<img src="{{ site.url }}{{ site.baseurl }}/images/bert/logo.jpg"  width="1000px" height='800px'/>

BRET is a revolutionary technique that achieved state-of-the-art results on a range of NLP tasks. 
Opposed to almost benchmarks trained with a given task, BRET is a self-supervised method, trained on unannotated text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BRET model can be fine tuned with just on additional output layer to create state-of-the-art models for a wide range of NLP applications such as Q-A models, language inference. 
   
   
There are normally two strategies for applying pre-trained language representations to down stream tasks:   
    _ **Feature-based** : to include the pre-trained representations as additional features.   
    _ **Fine-tuning** : retrain a minimal task-specific parameters on the downstream tasks.    
      
Released by Google in 2018. [BERT](https://github.com/google-research/bert) stands for Bidirectional Encoder Representation from Transformers.
  
There are two steps in implementation of BERT :    
    _ *Pre-training* in this step, the model will be trained on unlabeled data over different pre-training tasks.    
    _ *Fine-tuning* all of the parameters are first initialized with the pre-trained parameters and then fine-tuned using 
labeled data from the downstream task.   

BERT Model 's explications:  
_ **Model architecture** is a multi-layer bidirectional Transformer encoder based on [the Annotated Transformer](https://arxiv.org/abs/1706.03762).   
_ **Input/Output representation**: the input embedding is the sum of the token embeddings, segment embeddings, and position embeddings. The token vocabulary is [WordPiece embeddings](https://arxiv.org/pdf/1609.08144.pdf).  

<img src="{{ site.url }}{{ site.baseurl }}/images/bert/input_embedding.jpg"  width="700px" height='400px'/>      

_ **Pre-training BERT** uses two unsupervised tasks in order to optimize parameters :  
    1.  *Masked LM* in this task, the model will try to predict the target works that are randomly chosen as the masked tokens.   
    2.  *Next Sentence Prediction* the model is trained to predict the next sentence labeled as *IsNext* in dataset.   
    The corpus used for pre-training are the *BooksCorpus* (800M words) and English Wiki (2,500M works).  
    
_ **Fine-tuning BERT** is straightforward since the self attention mechanism in the Transformer. For each task, the input is fed into to the model, and then all parameters will be fine-tuned to predict specific output of this task. Depend on the task, sentence A and sentence B from pre-training are analogous to :   
    _ In paraphrasing A and B are sentence pairs.   
    _ In entailment A is hypothesis and B is premise.   
    _ In question answering, A, B are question-passage pairs.    
    
    
    
    

RoBERTa - an optimized method for BRET, builds on BRET's language masking strategy, 
wherein the system learns to predict intentionally hidden sections of text within otherwise unannotated language examples. 
It modifies key hyper-parameters in BRET: 
    . Remove BRET's next sentence pre-training objective.   
    . Train with larger mini-batches and learning rates. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.   

